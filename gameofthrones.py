# -*- coding: utf-8 -*-
"""GameofThrones.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BRaPFZ4pnLFd_mEsuz8eE8eCl5v91gPT
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

redditdata = pd.read_csv('/content/drive/MyDrive//Colab Notebooks//Assignment 3/GameofThrones.csv')

redditdata.isnull().sum()

redditdata.isna().sum()

redditdata.info()

redditdata.head()

import pandas as pd
import numpy as np
import json
import nltk
import re
import csv
import matplotlib.pyplot as plt 
import seaborn as sns
from tqdm import tqdm
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
redditdata.plot(kind='density',subplots=True,layout=(7,3),figsize=(16,12),sharex=False)
plt.show()

redditdata.plot(kind='box',subplots=True,layout=(7,3),figsize=(12,24),sharex=False)
plt.show()

pd.plotting.scatter_matrix(redditdata,figsize=(16,16))
plt.show()

# function for text cleaning 
def clean_text(text):
    # remove backslash-apostrophe 
    text = re.sub("\'", "", text) 
    # remove everything except alphabets 
    text = re.sub("[^a-zA-Z]"," ",text) 
    # remove whitespaces 
    text = ' '.join(text.split()) 
    # convert text to lowercase 
    text = text.lower() 
    
    return text

nltk.download('stopwords')

import nltk
nltk.download('treebank')
nltk.download('punkt')
sents = nltk.corpus.treebank_raw.sents()
print(len(sents))
for x in sents:
    print(x)

tokens = []
boundaries = set()
offset = 0
for sent in sents:
    tokens.extend(sent)
    offset += len(sent)
    boundaries.add(offset-1)

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from time import time

import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC

from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score,precision_recall_curve, auc

from matplotlib import pyplot

def remove_by_regex(texts, regexp):
        output_texts = regexp.sub("",texts)
        return output_texts

def remove_urls(input_text):
        return remove_by_regex(input_text, re.compile(r"http.?://[^\s]+[\s]?"))
    
def text_preprocessing(uncleaned_text_corpus):
    corpus = []
    for i in range(0, len(uncleaned_text_corpus)):
      review = re.sub('[^a-zA-Z]', ' ',uncleaned_text_corpus[i])
      review = review.lower()
      review = remove_urls(review)
      review = review.split()
      ps = PorterStemmer()
      all_stopwords = stopwords.words('english')
      all_stopwords.remove('not')
      review = [ps.stem(word) for word in review if not word in set(all_stopwords)]
      review = ' '.join(review)
      corpus.append(review)
    return corpus

cleaned_corpus = text_preprocessing('redditdata')

tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=1500)
X = tfidf_vectorizer.fit_transform(cleaned_corpus).toarray()
y = dataset.iloc[:, -1].values

cleaned_corpus = text_preprocessing('redditdata')

tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=1500)
X = tfidf_vectorizer.fit_transform(cleaned_corpus).toarray()
y = dataset.iloc[:, -1].values